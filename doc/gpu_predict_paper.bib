@InProceedings{SIG09,
	author = {Kayvon Fatahalian},
	series = {SIGGRAPH 2009},
	title = {{Beyond Programmable Shading: How Shader Cores Work}},
	year = {2009},
	url = {http://s09.idav.ucdavis.edu/}
}

@article{ispass09,
    abstract = {{Modern graphic processing units (GPUs) provide sufficiently flexible programming models that understanding their performance can provide insight in designing tomorrow's manycore processors, whether those are GPUs or otherwise. The combination of multiple, multithreaded, SIMD cores makes studying these GPUs useful in understanding tradeoffs among memory, data, and thread level parallelism. While modern GPUs offer orders of magnitude more raw computing power than contemporary CPUs, many important applications, even those with abundant data level parallelism, do not achieve peak performance. This paper characterizes several non-graphics applications written in NVIDIA's CUDA programming model by running them on a novel detailed microarchitecture performance simulator that runs NVIDIA's parallel thread execution (PTX) virtual instruction set. For this study, we selected twelve non-trivial CUDA applications demonstrating varying levels of performance improvement on GPU hardware (versus a CPU-only sequential version of the application). We study the performance of these applications on our GPU performance simulator with configurations comparable to contemporary high-end graphics cards. We characterize the performance impact of several microarchitecture design choices including choice of interconnect topology, use of caches, design of memory controller, parallel workload distribution mechanisms, and memory request coalescing hardware. Two observations we make are (1) that for the applications we study, performance is more sensitive to interconnect bisection bandwidth rather than latency, and (2) that, for nx4eZ5jT5DY3some applications, running fewer threads concurrently than on-chip resources might otherwise allow can improve performance by reducing contention in the memory system.}},
    author = {Bakhoda, A. and Yuan, G. L. and Fung, W. W. L. and Wong, H. and Aamodt, T. M.},
    booktitle = {Performance Analysis of Systems and Software, 2009. ISPASS 2009. IEEE International Symposium on},
    citeulike-article-id = {6859108},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ISPASS.2009.4919648},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4919648},
    doi = {10.1109/ISPASS.2009.4919648},
    isbn = {978-1-4244-4184-6},
    keywords = {gpgpu-sim},
    location = {Boston, MA, USA},
    month = apr,
    pages = {163--174},
    posted-at = {2010-09-07 11:17:15},
    priority = {5},
    publisher = {IEEE},
    title = {{Analyzing CUDA workloads using a detailed GPU simulator}},
    url = {http://dx.doi.org/10.1109/ISPASS.2009.4919648},
    year = {2009}
}

@InProceedings{SIG07,
	author = {John Owens},
	series = {SIGGRAPH 2007},
	title = {{GPGPU: GPU Architecture Overview}},
	year = {2007},
	url = {http://www.gpgpu.org/static/s2007/slides/02-gpu-architecture-overview-s07.pdf}
}

@article{Pan92,
 author = {Pan, Shien-Tai and So, Kimming and Rahmeh, Joseph T.},
 title = {Improving the accuracy of dynamic branch prediction using branch correlation},
 journal = {SIGPLAN Not.},
 volume = {27},
 issue = {9},
 month = {September},
 year = {1992},
 issn = {0362-1340},
 pages = {76--84},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/143371.143490},
 doi = {http://doi.acm.org/10.1145/143371.143490},
 acmid = {143490},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@InProceedings{Luebke08,
	author = {David Luebke},
	url = {http://s08.idav.ucdavis.edu/luebke-nvidia-gpu-architecture.pdf},
	title = {{GPU Architecture: Implications Trends}},
	year = {2008},
	series = {SIGGRAPH 2008}
}

@inproceedings{Zhang09,
  author = {He, Liqiang and Zhang, Guangyong},
  editor = {Zhang, Wu and Chen, Zhangxin and Douglas, Craig C. and Tong, Weiqin},
  booktitle = {HPCA (China)},
  pages = {153-160},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  title = {Parallel Branch Prediction on GPU Platform.},
  url = {http://dblp.uni-trier.de/db/conf/cnhpca/cnhpca2009.html#HeZ09},
  volume = 5938,
  year = 2009,
  isbn = {978-3-642-11841-8},
  biburl = {http://www.bibsonomy.org/bibtex/20a7e5049516339874f16f12801b3caae/dblp},
  date = {2010-03-19}
}

@INPROCEEDINGS{yags98,
    author = {A. N. Eden and T. Mudge {ane}},
    title = {The YAGS branch prediction scheme},
    booktitle = {In Proceedings of the 31st Annual ACM/IEEE International Symposium on Microarchitecture},
    year = {1998},
    pages = {69--77},
    url = {http://www.eecs.umich.edu/~tnm/papers/yags.pdf}
}

@inproceedings{Yeh91,
 author = {Yeh, Tse-Yu and Patt, Yale N.},
 title = {Two-level adaptive training branch prediction},
 booktitle = {Proceedings of the 24th annual international symposium on Microarchitecture},
 series = {MICRO 24},
 year = {1991},
 isbn = {0-89791-460-0},
 location = {Albuquerque, New Mexico, Puerto Rico},
 pages = {51--61},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/123465.123475},
 doi = {http://doi.acm.org/10.1145/123465.123475},
 acmid = {123475},
 publisher = {ACM},
 address = {New York, NY, USA}
} 
